---
title: "Predicting Average Daily Cost of Palliative Care and Identifying Key Determinants"
author: "Alex Furuya"
date: "6/1/2018"
output: html_document
---

### Introduction

Palliative care is an approach to health care for people with life-ending illness that focuses on quality of life and relieving symptoms rather than extending life. It is a multidisciplinary approach that is said to be a necessity for patients, yet, its cost can vary and sometimes be expensive. For this final project, I will be using the 2007 National Home and Hospice Care Survey in order to predict average daily cost of palliative, or end-of-life, care. In doing this, I hope to create a way for patients, doctors, administrators and stake holders to be know how much the care may cost, and to identify key variables that determine it. I will be using a multivariable regression model, several decision tree models, and finally a neural network to create a machine to predict average cost based on a number of variables. 

### Loading Library

For this project, I will be loading in the 'tidyverse' library and other libraries to help with decision trees and neural network.

```{r, eval = FALSE}
# Loading Library
library(tidyverse)
library(modelr)
library(boot)
library(broom)
library(leaps)
library(ggpubr)
library(glmnet)
library(tree)
library(randomForest)
library(gbm)
library(tensorflow)
use_python("")
library(keras)
```

### EDA

### Data Wrangling

I have already created the final data set in another project under the name 'Creating_Final_Dataset.rmd'. Here I will be formatting it to aid in the model creations.

```{r, eval=FALSE}
# Loading Data
hospice <- read_csv("final.csv") 

# Setting Variables to Factors
hospice[6:47] <- lapply(hospice[6:47], factor)

# Creating Training and Validation Set
set.seed(3)
train <- sample_n(tbl = hospice, replace = FALSE, size = 5400)
test <- setdiff(hospice, train)
```

# Multivariable Regression

Here we will create two multivariable regression models for both prediction and inferences. We will be creating one model with all the variables, and another model with selected variables using regsubet.

### Multivariable Regression with All Variables

```{r, eval=FALSE}
# Creating Raw Regression Model
fit <- glm(AVEDLYCH ~., data = train)

# Summarizing Raw Fit
summary(fit)

# Diagnostic Plot
plot(fit)

# MSE of Raw Fit
pred_fit <- predict(fit, test)
MSE_fit <- mean(pred_fit^2)
```

Here we see that the statistically significant variables in predicting average daily cost of care is total number of Activities of Daily Living that need assistance, rating of cognitive function, whether the patient is a current home health patient or a hospice discharge, whether or not the patient was a readmission, the race of patient, whether or not the patiet was enrolled in Medicare or Medicaid, whether or not the patient was an inpatient at health care center before hospice care, whether or not the patient used one or more personal care and therapy services, whether or not the patient used emergency service, the marital status of the patient, and if the patient's primary diagnosis was genitourinary or not. With this model, the test mean squared error was 21,953.98.

### Multivariable Regression with Subset Selection

```{r, eval=FALSE}
# Running regsubsets to Identify Most Important Variables
full_subset <- regsubsets(train$AVEDLYCH ~ .,train, nvmax = 10, really.big = T)

# Isolating Criterion Information of Subset Models
rss <- as.tibble(summary(full_subset)$rss)
colnames(rss) <- c("rss")
adjr2 <- as.tibble(summary(full_subset)$adjr2)
colnames(adjr2) <- c("adjr2")
cp <- as.tibble(summary(full_subset)$cp)
colnames(cp) <- c("cp")
bic <- as.tibble(summary(full_subset)$bic)
colnames(bic) <- c("bic")
variables <- as.tibble(seq.int(nrow(rss)))
colnames(variables) <- c("variables")

criteria <- bind_cols(variables, rss, adjr2, cp, bic)
criteria

# Identifying Model with Minimum BIC
which.min(criteria$bic)

# Identifying Coefficients in Previous Model
coef(full_subset ,7)

subset <- glm(AVEDLYCH ~ TOTALADL + COGNFUNC + PHTYPE + READMSS + INPATIENT + MARSTAT5 + CDDX1mental, data = train)

# MSE of Subset Selection
pred_subset <- predict(subset, test)
MSE_subset <- mean(pred_subset^2)
```

Using regsubset, we find that the average daily cost of care was statistically significant with the total number of activities of daily living that needed handicap, the level of cognitive funticon, whether the patient was a home care patient or a hospice patient, whether or not the patient was readmitted, whether or not the patient was an inpatient elsewere, the marital status of thepatient, and whether or not the primary diagnosis of the patient was mental health related. With this model, the test mean squared error was 21,841.16.

### Multivariable Regression Conclusion

Both regression models had similar mean squared errors, and had selected similar variables as statistically significant in predicting average daily cost of care, namely total numbers of activities of daily living that need assistance, the type of patient in terms of whether they are a home patient or a hospice patient, and whether or not the patient was a readmission patient.

# Trees

We will now create decision trees to predict the average cost of daily care. We will be creating one tree, bagged trees, random forests and boosted trees.

### One Decision Tree

```{r, eval=FALSE}
# Creating Decision Tree
tree <- tree(AVEDLYCH ~ ., data = train)

# Looking at Splitting Criterions
summary(tree)

# Visualizing the Tree
plot(tree) 
text(tree, pretty = 0)

# MSE of One Tree
pred_tree <- predict(tree, test)
MSE_tree <- mean(pred_tree^2)
```

With a single tree, we see that it only selected two factors: whether or not the patient was a home patient or a hospice patient and whether or not the patient received care prior to palliative care. With this model, the test mean squared error was 21,212.89.

### Bagging Decision Tree

```{r, eval=FALSE}
# Bagging 
bag <- randomForest(AVEDLYCH~ ., train, importance = T)

#Looking at the forest
plot(bag)

# MSE of Bagging Decision Tree
pred_bag <- predict(bag, test)
MSE_bag <- mean(pred_bag^2)
```



### Random Forest

```{r, eval=FALSE}
# Creating new forest with 25 trees
forest <- randomForest(AVEDLYCH ~ ., train, mtry = 10, ntree = 150)

# Importance of variables
importance(forest)

# Plotting importance
varImpPlot(forest)

# MSE of Random Forest
pred_forest <- predict(forest, test)
MSE_forest <- mean(pred_forest^2)
```

### Boosted Tree

```{r, eval=FALSE}
# Create the Boosted Tree
boost <- gbm(AVEDLYCH ~ ., data = train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4)

# Relative influence of each variable
summary(boost)

# Plotting relative influence
par(mfrow = c(1, 2))
  plot(boost, i = "PHTYPE")
  plot(boost, i = "INPATIENT")

# MSE of Bossted Tree
pred_boost <- predict(boost, test, n.trees = 5000)
MSE_boost <- mean(pred_boost^2)
```

### Neural Network

```{r, eval=FALSE}
#Creating Model Matrix for Training Dataset
x_train_num <- model.matrix(AVEDLYCH ~ TOTCDDX + TOTPROC + TOTALADL + COGNFUNC, data = train)[,-1]
x_train_fac <- model.matrix(AVEDLYCH ~ . -TOTCDDX - TOTPROC - TOTALADL - COGNFUNC, data = train)[,-1]
mean_train <- apply(x_train_num, 2, mean)
sd_train <- apply(x_train_num, 2, sd)
x_train_num <- scale(x_train_num, center = mean_train, scale = sd_train)
x_train <- cbind(x_train_num, x_train_fac)

##Creating Response for Training Dataset
y_train <- train %>% pull(AVEDLYCH)

#Creating Model Matrix for Testing Dataset
x_test_num <- model.matrix(AVEDLYCH ~ TOTCDDX + TOTPROC + TOTALADL + COGNFUNC, data = test)[,-1]
x_test_fac <- model.matrix(AVEDLYCH ~ . -TOTCDDX - TOTPROC - TOTALADL - COGNFUNC, data = test)[,-1]
mean_test <- apply(x_test_num, 2, mean)
sd_test <- apply(x_test_num, 2, sd)
x_test_num <- scale(x_test_num, center = mean_train, scale = sd_train)
x_test <- cbind(x_test_num, x_test_fac)

##Creating Response for Testing Dataset
y_test <- test %>% pull(AVEDLYCH)

# Creating Neural Network Model
build_model <- function() {
  model <- keras_model_sequential() %>% 
    layer_dense(units = 16, activation = "relu", 
                input_shape = dim(x_train)[[2]]) %>% 
    layer_dense(units = 16, activation = "relu") %>% 
    layer_dense(units = 1) 
    
  model %>% compile(
    optimizer = "rmsprop", 
    loss = "mse", 
    metrics = c("mse")
  )
}

# Setting Up for Cross Validation with Neural Network
k <- 4
indices <- sample(1:nrow(train))
folds <- cut(1:length(indices), breaks = k, labels = FALSE) 
num_epochs <- 20
all_scores <- c()

# Running Cross Validation with Neural Network
for (i in 1:k) {
  cat("processing fold #", i, "\n")
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE) 
  val_data <- x_train[val_indices,]
  val_targets <- y_train[val_indices]
  
  # Prepare the training data: data from all other partitions
  partial_train_data <- x_train[-val_indices,]
  partial_train_targets <- y_train[-val_indices]
  
  # Build the Keras model (already compiled)
  model <- build_model()
  
  # Train the model (in silent mode, verbose=0)
  model %>% fit(partial_train_data, partial_train_targets,
                epochs = num_epochs, batch_size = 1, verbose = 0)
                
  # Evaluate the model on the validation data
  results <- model %>% evaluate(val_data, val_targets, verbose = 0)
  all_scores <- c(all_scores, results$mean_square_error)
}

# Getting Test MSE
results <- model %>% evaluate(x_test, y_test)
MSE_neural <- results$mean_squared_error
```

### Comparison of Models

```{r, eval=FALSE}
# Compiling All MSEs from Previous Models
error <- tibble(MSE_fit_raw, MSE_fit_subset, MSE_tree, MSE_bag, MSE_forest, MSE_boost, MSE_neural)

# Displaying All MSEs from Previous Models
error
```
